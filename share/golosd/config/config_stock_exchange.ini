# Endpoint for P2P node to listen on
p2p-endpoint = 0.0.0.0:4243

# Maxmimum number of incoming connections on P2P endpoint
# p2p-max-connections =

# P2P nodes to connect to on startup (may specify multiple times)
# p2p-seed-node =

# Pairs of [BLOCK_NUM,BLOCK_ID] that should be enforced as checkpoints.
# checkpoint =

# Number of threads for rpc-clients. The optimal value is `<number of CPU>-1`
webserver-thread-pool-size = 2

# IP:PORT for HTTP connections
webserver-http-endpoint = 127.0.0.1:8090

# IP:PORT for WebSocket connections
webserver-ws-endpoint = 127.0.0.1:8091

# Maximum microseconds for trying to get read lock
read-wait-micro = 500000

# Maximum retries to get read lock. Each retry is read-wait-micro microseconds.
# When all retries are made, the rpc-client receives error 'Unable to acquire READ lock'.
max-read-wait-retries = 10

# Maximum microseconds for trying to get write lock on broadcast transaction.
write-wait-micro = 500000

# Maximum retries to get write lock. Each retry is write-wait-micro microseconds.
# When all retries are made, the rpc-client receives error 'Unable to acquire WRITE lock'.
max-write-wait-retries = 3

# Do all write operations (push_block/push_transaction) in the single thread.
# Write lock of database is very heavy. When many threads tries to lock database on writing, rpc-clients
# receive many errors 'Unable to acquire READ lock' ('Unable to acquire WRITE lock').
# Enabling of this options can increase performance.
single-write-thread = true

# Enable plugin notifications about operations in a pushed transaction, which should be included to the next generated
# block. Plugins doesn't validate data in operations, they only update its own indexes, so notifications can be
# disabled on push_transaction() without any side-effects. The option doesn't have effect on a pushing signed blocks,
# so it is safe.
# Disabling of this option can increase performance.
enable-plugins-on-push-transaction = false

# A start size for shared memory file when it doesn't have any data. Possible cases:
# - If shared memory has data and the value is greater then the size of shared_memory.bin,
#   the file will be grown to requested size.
# - If shared memory has data and the value is less then the size of shared_memory.bin, nothing happens.
# Changing of this parameter doesn't require the replaying.
shared-file-size = 2G

# The minimum free space in the shared memory file. When free space reaches the following value, the size of the
# shared_memory.bin increases by the value of inc-shared-file-size.
min-free-shared-file-size = 500M

# Step of increasing size of shared_memory.bin. When the free memory size reaches min-free-shared-file-size,
# the shared memory size increases by the following value.
inc-shared-file-size = 2G

# How often do checking the free space in shared_memory.bin. A very frequent checking can decrease performance.
# It's not critical if the free size became very small, because the daemon catches the `bad_alloc` exception
# and resizes. The optimal strategy is do checking of the free space, but not very often.
block-num-check-free-size = 1000 # each 3000 seconds

plugin = chain p2p json_rpc webserver network_broadcast_api witness database_api block_info raw_block account_history market_history

# Remove votes before defined block, should increase performance
clear-votes-before-block = 4294967295 # clear votes after each cashout

# Virtual operations will not be passed to the plugins, enabling of the option helps to save some memory.
skip-virtual-ops = true

# Defines a range of accounts to track by the account_history plugin as a json pair ["from","to"] [from,to]
# track-account-range =

# Defines a list of operations which will be explicitly logged by the account_history plugin.
# history-whitelist-ops =

# Defines a list of operations which will be explicitly ignored by the account_history plugin.
# history-blacklist-ops =

# Defines starting block from which recording stats by the account_history plugin.
# history-start-block =

# Track market history by grouping orders into buckets of equal size measured in seconds specified as a JSON array of numbers
bucket-size = [15,60,300,3600,86400]

# How far back in time to track history for each bucket size, measured in the number of buckets (default: 5760)
history-per-size = 5760

# Enable block production, even if the chain is stale.
enable-stale-production = false

# Percent of witnesses (0-99) that must be participating in order to produce blocks
required-participation = 0

# Number of threads to use for proof of work mining
mining-threads = 0

# declare an appender named "stderr" that writes messages to the console
[log.console_appender.stderr]
stream=std_error

# declare an appender named "p2p" that writes messages to p2p.log
[log.file_appender.p2p]
filename=logs/p2p/p2p.log
# filename can be absolute or relative to this config file

# route any messages logged to the default logger to the "stderr" logger we
# declared above, if they are info level are higher
[logger.default]
level=debug
appenders=stderr

# route messages sent to the "p2p" logger to stderr too
[logger.p2p]
level=error
appenders=stderr
